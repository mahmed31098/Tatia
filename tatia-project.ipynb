{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tatia Project: Named Entity Recognition","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import OneHotEncoder\nfrom transformers import BertTokenizer, BertForTokenClassification, TFBertModel \nfrom tensorflow.keras.utils import pad_sequences\nfrom tensorflow.keras import Input\nfrom tensorflow.keras import models, layers\nfrom tensorflow.keras import losses, optimizers, regularizers, callbacks\nfrom tensorflow.keras.utils import plot_model\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-24T23:14:33.917155Z","iopub.execute_input":"2024-01-24T23:14:33.917587Z","iopub.status.idle":"2024-01-24T23:14:54.186329Z","shell.execute_reply.started":"2024-01-24T23:14:33.917552Z","shell.execute_reply":"2024-01-24T23:14:54.185046Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\"conll2003\")","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-24T23:14:54.188503Z","iopub.execute_input":"2024-01-24T23:14:54.189286Z","iopub.status.idle":"2024-01-24T23:15:02.979843Z","shell.execute_reply.started":"2024-01-24T23:14:54.189247Z","shell.execute_reply":"2024-01-24T23:15:02.978864Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.58k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c43d3a6cbec49c283bdcbff3abb59aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/1.62k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca6fac8ff7eb4ae1bfa3b14a2accfc26"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset conll2003/conll2003 (download: 959.94 KiB, generated: 9.78 MiB, post-processed: Unknown size, total: 10.72 MiB) to /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/983k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c89c765b74a54b3ba3ee9c25fec7e7f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/14042 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3251 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/3454 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset conll2003 downloaded and prepared to /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bf47a18080f4eb79ee7fbf60013f35b"}},"metadata":{}}]},{"cell_type":"code","source":"num_classes = dataset[\"train\"].features[\"ner_tags\"].feature.num_classes\ncheckpoint_path = \"./ner_model-{epoch:02d}.weights.h5\"\nmax_len = 100\npatience = 5\nepochs = 4\nbatch_size = 128\nnumber_of_steps_to_save = int(dataset[\"train\"].num_rows / batch_size) * 2\n\ncallbacks_list = [\n    callbacks.EarlyStopping(monitor='val_accuracy', \n        mode='max',\n        patience=patience,\n        restore_best_weights=True),\n    callbacks.ModelCheckpoint(filepath=checkpoint_path,\n        monitor=\"val_accuracy\",\n        verbose=0,\n        save_best_only=False,\n        save_weights_only=True,\n        mode=\"auto\",\n        save_freq=number_of_steps_to_save)\n]","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-24T23:15:02.980988Z","iopub.execute_input":"2024-01-24T23:15:02.981322Z","iopub.status.idle":"2024-01-24T23:15:02.990494Z","shell.execute_reply.started":"2024-01-24T23:15:02.981295Z","shell.execute_reply":"2024-01-24T23:15:02.988935Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def tokenize_and_split(hugging_face_dataset, tokenizer):\n    tokens = []\n    labels = []\n    cols = [\"tokens\", \"ner_tags\"]\n    for line in hugging_face_dataset:\n        ner_tags_list = line.get(cols[1])\n        tokens.append(\"[CLS]\")\n        labels.append(0)\n        for i, word in enumerate(line.get(cols[0])):\n            token_list = tokenizer.tokenize(word)\n            tokens.extend(token_list)\n            labels.extend([ner_tags_list[i]] * len(token_list))\n        tokens.append(\"[SEP]\")\n        labels.append(0)\n    return tokens, labels","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-24T23:15:02.993059Z","iopub.execute_input":"2024-01-24T23:15:02.993434Z","iopub.status.idle":"2024-01-24T23:15:03.006698Z","shell.execute_reply.started":"2024-01-24T23:15:02.993405Z","shell.execute_reply":"2024-01-24T23:15:03.005718Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# def preprocess(hugging_face_dataset):\n#     tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n#     tokens, labels = tokenize_and_split(hugging_face_dataset, tokenizer)\n\n#     input_ids = tokenizer.convert_tokens_to_ids(tokens)\n#     print(\"input_ids: \", input_ids)\n#     segment_ids = [0] * len(tokens)\n#     attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]\n#     return tokens, labels, input_ids, segment_ids, attention_masks","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-24T23:15:03.008037Z","iopub.execute_input":"2024-01-24T23:15:03.008601Z","iopub.status.idle":"2024-01-24T23:15:03.019165Z","shell.execute_reply.started":"2024-01-24T23:15:03.008560Z","shell.execute_reply":"2024-01-24T23:15:03.018143Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def prepare_label_enc():\n    label_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False, dtype=int)\n    classes = np.array([i for i in range(num_classes)])\n    label_encoder.fit_transform(classes.reshape(-1, 1))\n    return label_encoder\n\ndef encode_labels(label_enc, labels: list[int]):\n    np_labels = np.array(labels, dtype=int)\n    return label_enc.transform(np_labels.reshape(-1, 1))\n","metadata":{"execution":{"iopub.status.busy":"2024-01-24T23:15:03.020503Z","iopub.execute_input":"2024-01-24T23:15:03.020832Z","iopub.status.idle":"2024-01-24T23:15:03.036659Z","shell.execute_reply.started":"2024-01-24T23:15:03.020805Z","shell.execute_reply":"2024-01-24T23:15:03.035664Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def align_encoded_labels_with_tokens(tokens: list[str], labels: list[int], label_enc):\n    \"\"\"\n        completes the labels so that they match the tokenize sentence\n    \"\"\"\n    aligned_labels = []\n    token_idx = 0\n    for token in tokens[1:-1]:\n        # Check if the token is a subword piece (starts with \"##\")\n        if token.startswith(\"##\") and token_idx > 0:\n            # If it's a subword, use the label of the previous token\n            aligned_labels.append(labels[token_idx - 1])\n        else:\n            # If it's not a subword, use the label of the current token\n            aligned_labels.append(labels[token_idx])\n            token_idx += 1\n    \n    # [CLS] separator\n    aligned_labels.insert(0, 0)\n    # [SEP] separator\n    aligned_labels.append(0)\n\n    return encode_labels(label_enc, aligned_labels) ","metadata":{"execution":{"iopub.status.busy":"2024-01-24T23:15:03.037906Z","iopub.execute_input":"2024-01-24T23:15:03.038302Z","iopub.status.idle":"2024-01-24T23:15:03.047988Z","shell.execute_reply.started":"2024-01-24T23:15:03.038272Z","shell.execute_reply":"2024-01-24T23:15:03.046982Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def preprocess(hugging_face_dataset):\n\n    tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n    label_enc = prepare_label_enc()\n    X_train = []\n    X_val = []\n    X_test = []\n    y_train = []\n    y_val = []\n    y_test = []\n    y_train_dec = []\n    y_val_dec = []\n    y_test_dec = []\n    for line in hugging_face_dataset[\"train\"]:\n        if line[\"tokens\"] == []:\n            continue\n        sentence = tokenizer.encode_plus(\n            text=line[\"tokens\"],\n            add_special_tokens=True,\n            max_length=max_len,\n            truncation=True,\n            padding=True,\n            return_tensors='np',\n            return_token_type_ids=False,\n            return_attention_mask=True,\n            verbose=True\n        )\n        X_train.append(sentence)\n        y_train_dec.append(line[\"ner_tags\"])\n        y_train.append(\n            align_encoded_labels_with_tokens(\n                tokenizer.convert_ids_to_tokens(sentence[\"input_ids\"].flatten()), \n                line[\"ner_tags\"], \n                label_enc\n            )\n        )\n        \n    for line in hugging_face_dataset[\"validation\"]:\n        if line[\"tokens\"] == []:\n            continue\n        sentence = tokenizer.encode_plus(\n            text=line[\"tokens\"],\n            add_special_tokens=True,\n            max_length=max_len,\n            truncation=True,\n            padding=True,\n            return_tensors='np',\n            return_token_type_ids=False,\n            return_attention_mask=True,\n            verbose=True\n        )\n        X_val.append(sentence)\n        y_val_dec.append(line[\"ner_tags\"])\n        y_val.append(\n            align_encoded_labels_with_tokens(\n                tokenizer.convert_ids_to_tokens(sentence[\"input_ids\"].flatten()), \n                line[\"ner_tags\"], \n                label_enc\n            )\n        )\n        \n    for line in hugging_face_dataset[\"test\"]:\n        if line[\"tokens\"] == []:\n            continue\n        sentence = tokenizer.encode_plus(\n            text=line[\"tokens\"],\n            add_special_tokens=True,\n            max_length=max_len,\n            truncation=True,\n            padding=True,\n            return_tensors='np',\n            return_token_type_ids=False,\n            return_attention_mask=True,\n            verbose=True\n        )\n        X_test.append(sentence)\n        y_test_dec.append(line[\"ner_tags\"])\n        y_test.append(\n            align_encoded_labels_with_tokens(\n                tokenizer.convert_ids_to_tokens(sentence[\"input_ids\"].flatten()), \n                line[\"ner_tags\"], \n                label_enc\n            )\n        )\n        \n    return X_train, X_val, X_test, y_train, y_val, y_test, y_train_dec, y_val_dec, y_test_dec\n#         print(type(X_train))\n#         print(X_train)\n#     X_train = tokenizer.batch_encode_plus(\n#             batch_text_or_text_pairs=hugging_face_dataset[\"train\"][\"tokens\"],\n#             add_special_tokens=True,\n#             max_length=max_len,\n#             truncation=True,\n#             padding=True,\n#             return_tensors='np',\n#             return_token_type_ids=False,\n#             return_attention_mask=True,\n#             verbose=True\n#         )\n#     return X_train\n    \n#     X_val = tokenizer(\n#         text=hugging_face_dataset[\"validation\"][\"tokens\"],\n#         add_special_tokens=True,\n#         max_length=max_len,\n#         truncation=True,\n#         padding=True,\n#         return_tensors='tf',\n#         return_token_type_ids=False,\n#         return_attention_mask=True,\n#         verbose=True\n#     )\n    \n#     X_test = tokenizer(\n#         text=hugging_face_dataset[\"test\"][\"tokens\"],\n#         add_special_tokens=True,\n#         max_length=max_len,\n#         truncation=True,\n#         padding=True,\n#         return_tensors='tf',\n#         return_token_type_ids=False,\n#         return_attention_mask=True,\n#         verbose=True\n#     )\n#     return X_train, X_val, X_test","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-24T23:15:03.049562Z","iopub.execute_input":"2024-01-24T23:15:03.049974Z","iopub.status.idle":"2024-01-24T23:15:03.067890Z","shell.execute_reply.started":"2024-01-24T23:15:03.049941Z","shell.execute_reply":"2024-01-24T23:15:03.066621Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"X_train, X_val, X_test, y_train, y_val, y_test, y_train_dec, y_val_dec, y_test_dec = preprocess(dataset)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T23:15:03.069271Z","iopub.execute_input":"2024-01-24T23:15:03.069613Z","iopub.status.idle":"2024-01-24T23:15:23.407267Z","shell.execute_reply.started":"2024-01-24T23:15:03.069584Z","shell.execute_reply":"2024-01-24T23:15:23.406384Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ba50f075c3a4f1c8b2437dad4213e6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7ef0631b42d46f3aeec0afc93de5755"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c001bfc2fcdb4a1d8de68a7e60004758"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13c37edae6ef43f88429a634661dcf10"}},"metadata":{}}]},{"cell_type":"code","source":"y_train[:2]","metadata":{"execution":{"iopub.status.busy":"2024-01-24T23:15:23.411204Z","iopub.execute_input":"2024-01-24T23:15:23.411555Z","iopub.status.idle":"2024-01-24T23:15:23.419501Z","shell.execute_reply.started":"2024-01-24T23:15:23.411529Z","shell.execute_reply":"2024-01-24T23:15:23.418678Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"[array([[1, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 1, 0, 0, 0, 0, 0],\n        [1, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 1, 0],\n        [1, 0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 1, 0],\n        [1, 0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 0, 0, 0, 0, 0, 0, 0, 0]]),\n array([[1, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 1, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 1, 0, 0, 0, 0, 0, 0],\n        [1, 0, 0, 0, 0, 0, 0, 0, 0]])]"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\nfor sent in X_train[:2]:\n    print(tokenizer.batch_decode(sent[\"input_ids\"]))","metadata":{"execution":{"iopub.status.busy":"2024-01-24T23:15:23.420630Z","iopub.execute_input":"2024-01-24T23:15:23.420912Z","iopub.status.idle":"2024-01-24T23:15:23.536475Z","shell.execute_reply.started":"2024-01-24T23:15:23.420888Z","shell.execute_reply":"2024-01-24T23:15:23.535307Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"['[CLS] EU rejects German call to boycott British [UNK]. [SEP]']\n['[CLS] Peter Blackburn [SEP]']\n","output_type":"stream"}]},{"cell_type":"code","source":"# _, labels_train, input_ids_train, segment_ids_train, attention_masks_train = preprocess(dataset[\"train\"])\n# _, labels_val, input_ids_val, segment_ids_val, attention_masks_val = preprocess(dataset[\"validation\"])\n# _, labels_test, input_ids_test, segment_ids_test, attention_masks_test = preprocess(dataset[\"test\"])","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-24T19:55:56.803279Z","iopub.execute_input":"2024-01-24T19:55:56.803585Z","iopub.status.idle":"2024-01-24T19:55:56.807605Z","shell.execute_reply.started":"2024-01-24T19:55:56.803561Z","shell.execute_reply":"2024-01-24T19:55:56.806681Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def prepare_batches(tokenized_sentences, aligned_encoded_labels, batch_size, is_test: bool = False):    \n    # Extract input_ids and attention_mask lists\n    input_ids_list = [sentence['input_ids'].flatten() for sentence in tokenized_sentences]\n    attention_mask_list = [sentence['attention_mask'].flatten() for sentence in tokenized_sentences]\n    # Calculate max_len dynamically based on the maximum sequence length in your data\n    max_len_list = max(len(ids) for ids in input_ids_list)\n    \n    # Pad sequences to the same length within each batch\n    input_ids_padded = pad_sequences(input_ids_list, maxlen=max_len_list, padding='post', value=0)\n    attention_mask_padded = pad_sequences(attention_mask_list, maxlen=max_len_list, padding='post', value=0)\n    aligned_encoded_labels_padded = pad_sequences(aligned_encoded_labels, maxlen=max_len_list, padding='post', value=0)\n    \n    # Convert to TensorFlow tensors\n    input_ids = tf.convert_to_tensor(input_ids_padded, dtype=tf.int32)\n    attention_mask = tf.convert_to_tensor(attention_mask_padded, dtype=tf.int32)\n    labels = tf.convert_to_tensor(aligned_encoded_labels_padded, dtype=tf.int32)\n\n    # Create a TensorFlow dataset\n    if not is_test:\n        dataset = tf.data.Dataset.from_tensor_slices(((input_ids, attention_mask), labels))\n    else:\n        dataset = tf.data.Dataset.from_tensor_slices((input_ids, attention_mask))\n\n    # Batch the dataset\n    dataset = dataset.batch(batch_size)\n\n    return dataset, max_len_list","metadata":{"execution":{"iopub.status.busy":"2024-01-24T23:15:23.537830Z","iopub.execute_input":"2024-01-24T23:15:23.538160Z","iopub.status.idle":"2024-01-24T23:15:23.548580Z","shell.execute_reply.started":"2024-01-24T23:15:23.538131Z","shell.execute_reply":"2024-01-24T23:15:23.547607Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def prepare_dataset(tokenized_sentences, aligned_encoded_labels):\n    # Extract input_ids and attention_mask lists\n    input_ids_list = [sentence['input_ids'].flatten() for sentence in tokenized_sentences]\n    attention_mask_list = [sentence['attention_mask'].flatten() for sentence in tokenized_sentences]\n    # Calculate max_len dynamically based on the maximum sequence length in your data\n    max_len_list = max(len(ids) for ids in input_ids_list)\n    \n    # Pad sequences to the same length within each batch\n    input_ids_padded = pad_sequences(input_ids_list, maxlen=max_len_list, padding='post', value=0)\n    attention_mask_padded = pad_sequences(attention_mask_list, maxlen=max_len_list, padding='post', value=0)\n    aligned_encoded_labels_padded = pad_sequences(aligned_encoded_labels, maxlen=max_len_list, padding='post', value=0)\n    \n    data = {}\n    data[\"input_ids\"] = input_ids_padded\n    data[\"attention_mask\"] = attention_mask_padded\n    data[\"labels\"] = aligned_encoded_labels_padded\n    return data","metadata":{"execution":{"iopub.status.busy":"2024-01-24T23:15:23.549900Z","iopub.execute_input":"2024-01-24T23:15:23.550256Z","iopub.status.idle":"2024-01-24T23:15:23.561387Z","shell.execute_reply.started":"2024-01-24T23:15:23.550203Z","shell.execute_reply":"2024-01-24T23:15:23.560480Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# train_dataset, max_len_train_list = prepare_batches(X_train, y_train, batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T19:55:56.832206Z","iopub.execute_input":"2024-01-24T19:55:56.832507Z","iopub.status.idle":"2024-01-24T19:55:56.844575Z","shell.execute_reply.started":"2024-01-24T19:55:56.832483Z","shell.execute_reply":"2024-01-24T19:55:56.843781Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# val_dataset, max_len_val_list = prepare_batches(X_val, y_val, batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T19:55:56.847274Z","iopub.execute_input":"2024-01-24T19:55:56.847524Z","iopub.status.idle":"2024-01-24T19:55:56.853655Z","shell.execute_reply.started":"2024-01-24T19:55:56.847503Z","shell.execute_reply":"2024-01-24T19:55:56.852969Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# test_dataset, max_len_test_list = prepare_batches(X_test, y_test, batch_size,is_test=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T19:55:56.854612Z","iopub.execute_input":"2024-01-24T19:55:56.854939Z","iopub.status.idle":"2024-01-24T19:55:56.862171Z","shell.execute_reply.started":"2024-01-24T19:55:56.854908Z","shell.execute_reply":"2024-01-24T19:55:56.861251Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"train_dataset = prepare_dataset(X_train, y_train)\nval_dataset = prepare_dataset(X_val, y_val)\ntest_dataset = prepare_dataset(X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T23:15:23.562543Z","iopub.execute_input":"2024-01-24T23:15:23.562916Z","iopub.status.idle":"2024-01-24T23:15:23.970145Z","shell.execute_reply.started":"2024-01-24T23:15:23.562889Z","shell.execute_reply":"2024-01-24T23:15:23.969230Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# test = TFBertModel.from_pretrained('bert-base-cased')\n# out = test(Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\"), Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\"))\n# out[-2]\n","metadata":{"execution":{"iopub.status.busy":"2024-01-24T19:55:57.218286Z","iopub.execute_input":"2024-01-24T19:55:57.218583Z","iopub.status.idle":"2024-01-24T19:55:57.222452Z","shell.execute_reply.started":"2024-01-24T19:55:57.218558Z","shell.execute_reply":"2024-01-24T19:55:57.221599Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# out[-2]","metadata":{"execution":{"iopub.status.busy":"2024-01-24T19:55:57.223500Z","iopub.execute_input":"2024-01-24T19:55:57.223786Z","iopub.status.idle":"2024-01-24T19:55:57.233244Z","shell.execute_reply.started":"2024-01-24T19:55:57.223757Z","shell.execute_reply":"2024-01-24T19:55:57.232354Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def fine_tune_bert(num_neuron_output: int):\n    bert_base = TFBertModel.from_pretrained('bert-base-cased')\n\n    input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n    print()\n    bert_base.pop()\n#     x = bert_base(input_ids, attention_mask = input_mask)[0]\n    output = layers.Dense(num_neuron_output, activation=\"softmax\")(x)\n    model = models.Model(inputs=[input_ids, input_mask], outputs=output, name= \"ner_model\")\n    return model\n\ndef build_model(num_hidden_layers: int, size_hidden_layers: list[int], hidden_activation_func: list[str], num_neuron_output: int):\n    if len(size_hidden_layers) != num_hidden_layers or len(hidden_activation_func) != num_hidden_layers:\n        raise Exception(\"The params num_hidden_layers and hidden_activation_func should have a length equal to num_hidden_layers\")\n    \n    bert_base = TFBertModel.from_pretrained('bert-base-cased')\n\n    input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n    x = bert_base(input_ids, attention_mask = input_mask)[0]\n#     x = out[-2]\n    for i in range(num_hidden_layers):\n        x = layers.Dense(size_hidden_layers[i], activation=hidden_activation_func[i])(x)\n        if i < num_hidden_layers - 2:\n            x = layers.Dropout(0.2)(x)\n    output = layers.Dense(num_neuron_output, activation=\"softmax\")(x)\n    model = models.Model(inputs=[input_ids, input_mask], outputs=output, name= \"ner_model\")\n    return model\n    \ndef configure_model(model, optimizer: str, loss_fn: str):\n    model.compile(\n        optimizer=optimizer, \n        loss=loss_fn,\n        metrics = \"accuracy\",\n    )\n    return model","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-24T23:15:23.971469Z","iopub.execute_input":"2024-01-24T23:15:23.971802Z","iopub.status.idle":"2024-01-24T23:15:23.983978Z","shell.execute_reply.started":"2024-01-24T23:15:23.971774Z","shell.execute_reply":"2024-01-24T23:15:23.982950Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"model = build_model(2, [20, 8], [\"relu\", \"relu\"], num_classes)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T23:15:23.985636Z","iopub.execute_input":"2024-01-24T23:15:23.986018Z","iopub.status.idle":"2024-01-24T23:15:37.764079Z","shell.execute_reply.started":"2024-01-24T23:15:23.985990Z","shell.execute_reply":"2024-01-24T23:15:37.763175Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5122fc65171d4cd390fef55d1d719bd5"}},"metadata":{}},{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"code","source":"# model = build_model(0, [], [], num_classes)\nmodel = configure_model(model, \"adam\", \"categorical_crossentropy\")","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-24T23:15:37.765350Z","iopub.execute_input":"2024-01-24T23:15:37.765662Z","iopub.status.idle":"2024-01-24T23:15:37.792008Z","shell.execute_reply.started":"2024-01-24T23:15:37.765636Z","shell.execute_reply":"2024-01-24T23:15:37.790984Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# history = model.fit(\n#     x = {'inputs_ids': input_ids_train, \"attention_mask\": attention_masks_train},\n#     y = labels_train,\n#     validation_data = ({\"input_ids\": input_ids_val, \"attention_mask\": attention_masks_val}, labels_val),\n#     epochs=epochs, batch_size=batch_size, callbacks=callbacks_list, \n    \n# )","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-22T15:03:40.794873Z","iopub.execute_input":"2024-01-22T15:03:40.795467Z","iopub.status.idle":"2024-01-22T15:03:40.799554Z","shell.execute_reply.started":"2024-01-22T15:03:40.795435Z","shell.execute_reply":"2024-01-22T15:03:40.798629Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# predicted = model.predict({'input_ids': input_ids_test, 'attention_mask': attention_masks_test})\n# labels_predicted = np.argmax(predicted, axis=1)\n# print(classification_report(labels_test, labels_predicted))","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-22T15:03:40.926052Z","iopub.execute_input":"2024-01-22T15:03:40.926370Z","iopub.status.idle":"2024-01-22T15:03:40.930481Z","shell.execute_reply.started":"2024-01-22T15:03:40.926345Z","shell.execute_reply":"2024-01-22T15:03:40.929446Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-01-24T23:15:37.793289Z","iopub.execute_input":"2024-01-24T23:15:37.793590Z","iopub.status.idle":"2024-01-24T23:15:37.852714Z","shell.execute_reply.started":"2024-01-24T23:15:37.793564Z","shell.execute_reply":"2024-01-24T23:15:37.851718Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Model: \"ner_model\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n input_ids (InputLayer)      [(None, 100)]                0         []                            \n                                                                                                  \n attention_mask (InputLayer  [(None, 100)]                0         []                            \n )                                                                                                \n                                                                                                  \n tf_bert_model (TFBertModel  TFBaseModelOutputWithPooli   1083102   ['input_ids[0][0]',           \n )                           ngAndCrossAttentions(last_   72         'attention_mask[0][0]']      \n                             hidden_state=(None, 100, 7                                           \n                             68),                                                                 \n                              pooler_output=(None, 768)                                           \n                             , past_key_values=None, hi                                           \n                             dden_states=None, attentio                                           \n                             ns=None, cross_attentions=                                           \n                             None)                                                                \n                                                                                                  \n dense (Dense)               (None, 100, 20)              15380     ['tf_bert_model[0][0]']       \n                                                                                                  \n dense_1 (Dense)             (None, 100, 8)               168       ['dense[0][0]']               \n                                                                                                  \n dense_2 (Dense)             (None, 100, 9)               81        ['dense_1[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 108325901 (413.23 MB)\nTrainable params: 108325901 (413.23 MB)\nNon-trainable params: 0 (0.00 Byte)\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"history = model.fit(\n    x = {\"input_ids\": train_dataset[\"input_ids\"] , \"attention_mask\": train_dataset[\"attention_mask\"]},\n    y = train_dataset[\"labels\"],\n    validation_data = ({\"input_ids\": val_dataset[\"input_ids\"] , \"attention_mask\": val_dataset[\"attention_mask\"]}, val_dataset[\"labels\"]),\n    epochs=epochs, batch_size=batch_size, callbacks=callbacks_list, \n)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T23:15:37.854062Z","iopub.execute_input":"2024-01-24T23:15:37.854499Z","iopub.status.idle":"2024-01-24T23:27:25.959949Z","shell.execute_reply.started":"2024-01-24T23:15:37.854459Z","shell.execute_reply":"2024-01-24T23:27:25.958885Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Epoch 1/4\n110/110 [==============================] - 221s 1s/step - loss: 0.3128 - accuracy: 0.0035 - val_loss: 0.3261 - val_accuracy: 7.9077e-04\nEpoch 2/4\n107/110 [============================>.] - ETA: 4s - loss: 0.3787 - accuracy: 0.0122","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/tf_utils.py:465: UserWarning: `seed_generator` is deprecated and will be removed in a future version.\n  warnings.warn(\"`seed_generator` is deprecated and will be removed in a future version.\", UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"110/110 [==============================] - 164s 1s/step - loss: 0.3774 - accuracy: 0.0120 - val_loss: 0.2700 - val_accuracy: 0.9735\nEpoch 3/4\n110/110 [==============================] - 160s 1s/step - loss: 0.3224 - accuracy: 0.1468 - val_loss: 0.8079 - val_accuracy: 0.0023\nEpoch 4/4\n110/110 [==============================] - 164s 1s/step - loss: 2.3820 - accuracy: 0.2618 - val_loss: 4.7132 - val_accuracy: 0.0028\n","output_type":"stream"}]},{"cell_type":"code","source":"# history = model.fit(\n#     train_dataset,\n#     validation_data = val_dataset,\n#     epochs=epochs, batch_size=batch_size, callbacks=callbacks_list, \n# )","metadata":{"execution":{"iopub.status.busy":"2024-01-24T23:27:51.021823Z","iopub.execute_input":"2024-01-24T23:27:51.022284Z","iopub.status.idle":"2024-01-24T23:27:51.027423Z","shell.execute_reply.started":"2024-01-24T23:27:51.022243Z","shell.execute_reply":"2024-01-24T23:27:51.026296Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"predicted = model.predict({\"input_ids\": test_dataset[\"input_ids\"] , \"attention_mask\": test_dataset[\"attention_mask\"]})\ny_pred = np.argmax(predicted, axis=2)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-24T23:28:15.159094Z","iopub.execute_input":"2024-01-24T23:28:15.159865Z","iopub.status.idle":"2024-01-24T23:28:30.429265Z","shell.execute_reply.started":"2024-01-24T23:28:15.159829Z","shell.execute_reply":"2024-01-24T23:28:30.428194Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"108/108 [==============================] - 15s 139ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"predicted","metadata":{"execution":{"iopub.status.busy":"2024-01-24T23:28:30.431025Z","iopub.execute_input":"2024-01-24T23:28:30.431392Z","iopub.status.idle":"2024-01-24T23:28:30.443008Z","shell.execute_reply.started":"2024-01-24T23:28:30.431362Z","shell.execute_reply":"2024-01-24T23:28:30.441790Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"array([[[2.5639673e-09, 1.6873159e-12, 4.3568453e-03, ...,\n         2.6110591e-31, 9.9564314e-01, 4.3963187e-08],\n        [2.5639673e-09, 1.6840236e-12, 4.3568453e-03, ...,\n         2.6110591e-31, 9.9564314e-01, 4.3963187e-08],\n        [2.5639673e-09, 1.6873159e-12, 4.3568453e-03, ...,\n         2.6110591e-31, 9.9564314e-01, 4.3963187e-08],\n        ...,\n        [2.5639673e-09, 1.6873159e-12, 4.3568453e-03, ...,\n         2.6110591e-31, 9.9564314e-01, 4.3963187e-08],\n        [2.5639673e-09, 1.6840236e-12, 4.3568453e-03, ...,\n         2.6110591e-31, 9.9564314e-01, 4.3963187e-08],\n        [2.5639673e-09, 1.6873159e-12, 4.3568453e-03, ...,\n         2.6110591e-31, 9.9564314e-01, 4.3963187e-08]],\n\n       [[2.5639673e-09, 1.6873159e-12, 4.3568453e-03, ...,\n         2.6110591e-31, 9.9564314e-01, 4.3963187e-08],\n        [2.5639673e-09, 1.6873159e-12, 4.3568453e-03, ...,\n         2.6110591e-31, 9.9564314e-01, 4.3963187e-08],\n        [2.5639673e-09, 1.6873159e-12, 4.3568453e-03, ...,\n         2.6161638e-31, 9.9564314e-01, 4.3963187e-08],\n        ...,\n        [2.5639673e-09, 1.6873159e-12, 4.3568453e-03, ...,\n         2.6110591e-31, 9.9564314e-01, 4.3963187e-08],\n        [2.5639673e-09, 1.6873159e-12, 4.3568453e-03, ...,\n         2.6110591e-31, 9.9564314e-01, 4.3963187e-08],\n        [2.5639673e-09, 1.6873159e-12, 4.3568453e-03, ...,\n         2.6110591e-31, 9.9564314e-01, 4.3963187e-08]],\n\n       [[2.5639673e-09, 1.6873159e-12, 4.3568453e-03, ...,\n         2.6110591e-31, 9.9564314e-01, 4.3963187e-08],\n        [2.5639673e-09, 1.6873159e-12, 4.3568453e-03, ...,\n         2.6110591e-31, 9.9564314e-01, 4.3963187e-08],\n        [2.5639673e-09, 1.6873159e-12, 4.3568453e-03, ...,\n         2.6110591e-31, 9.9564314e-01, 4.3963187e-08],\n        ...,\n        [2.5639673e-09, 1.6873159e-12, 4.3568453e-03, ...,\n         2.6110591e-31, 9.9564314e-01, 4.3963187e-08],\n        [2.5639673e-09, 1.6873159e-12, 4.3568453e-03, ...,\n         2.6161638e-31, 9.9564314e-01, 4.3963187e-08],\n        [2.5639673e-09, 1.6873159e-12, 4.3568453e-03, ...,\n         2.6110591e-31, 9.9564314e-01, 4.3963187e-08]],\n\n       ...,\n\n       [[2.5639673e-09, 1.6873159e-12, 4.3568453e-03, ...,\n         2.6110591e-31, 9.9564314e-01, 4.3963187e-08],\n        [2.5639673e-09, 1.6873159e-12, 4.3568453e-03, ...,\n         2.6110591e-31, 9.9564314e-01, 4.3963187e-08],\n        [2.5639673e-09, 1.6873159e-12, 4.3568453e-03, ...,\n         2.6161638e-31, 9.9564314e-01, 4.3963187e-08],\n        ...,\n        [2.5639673e-09, 1.6840236e-12, 4.3568453e-03, ...,\n         2.6110591e-31, 9.9564314e-01, 4.3963187e-08],\n        [2.5639673e-09, 1.6840236e-12, 4.3568453e-03, ...,\n         2.6110591e-31, 9.9564314e-01, 4.3963187e-08],\n        [2.5639673e-09, 1.6873159e-12, 4.3568453e-03, ...,\n         2.6110591e-31, 9.9564314e-01, 4.3963187e-08]],\n\n       [[2.5639673e-09, 1.6873159e-12, 4.3568453e-03, ...,\n         2.6110591e-31, 9.9564314e-01, 4.3963187e-08],\n        [2.5639673e-09, 1.6873159e-12, 4.3568453e-03, ...,\n         2.6110591e-31, 9.9564314e-01, 4.3963187e-08],\n        [2.5639673e-09, 1.6873159e-12, 4.3568453e-03, ...,\n         2.6110591e-31, 9.9564314e-01, 4.3963187e-08],\n        ...,\n        [2.5639673e-09, 1.6840236e-12, 4.3568453e-03, ...,\n         2.6110591e-31, 9.9564314e-01, 4.3963187e-08],\n        [2.5639673e-09, 1.6873159e-12, 4.3568453e-03, ...,\n         2.6110591e-31, 9.9564314e-01, 4.3963187e-08],\n        [2.5639673e-09, 1.6873159e-12, 4.3568453e-03, ...,\n         2.6161638e-31, 9.9564314e-01, 4.3963187e-08]],\n\n       [[2.5639673e-09, 1.6840236e-12, 4.3568453e-03, ...,\n         2.6110591e-31, 9.9564314e-01, 4.3963187e-08],\n        [2.5639673e-09, 1.6873159e-12, 4.3568453e-03, ...,\n         2.6110591e-31, 9.9564314e-01, 4.3963187e-08],\n        [2.5639673e-09, 1.6840236e-12, 4.3568453e-03, ...,\n         2.6110591e-31, 9.9564314e-01, 4.3963187e-08],\n        ...,\n        [2.5639673e-09, 1.6873159e-12, 4.3568453e-03, ...,\n         2.6110591e-31, 9.9564314e-01, 4.3963187e-08],\n        [2.5639673e-09, 1.6840236e-12, 4.3568453e-03, ...,\n         2.6110591e-31, 9.9564314e-01, 4.3963187e-08],\n        [2.5639673e-09, 1.6873159e-12, 4.3568453e-03, ...,\n         2.6110591e-31, 9.9564314e-01, 4.3963187e-08]]], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"# def preprocess_y_test(align_labels, max_len):\n#     return pad_sequences(align_labels, maxlen=max_len, padding='post', value=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def postprocess(labels_pred:list[list[int]], y_true: list[list[int]]):\n    labels_pred_cut = []\n    for i, sentence in enumerate(y_true):\n        sen_len = len(sentence)\n        labels_pred_cut.append(labels_pred[i][:sen_len])\n    return labels_pred_cut","metadata":{"execution":{"iopub.status.busy":"2024-01-24T20:29:06.435183Z","iopub.execute_input":"2024-01-24T20:29:06.435820Z","iopub.status.idle":"2024-01-24T20:29:06.441012Z","shell.execute_reply.started":"2024-01-24T20:29:06.435787Z","shell.execute_reply":"2024-01-24T20:29:06.440051Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"postprocess(y_pred, y_test_dec)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test_dec = np.array(y_test_dec, dtype=int)","metadata":{"execution":{"iopub.status.busy":"2024-01-21T17:07:25.217364Z","iopub.execute_input":"2024-01-21T17:07:25.217767Z","iopub.status.idle":"2024-01-21T17:07:25.263289Z","shell.execute_reply.started":"2024-01-21T17:07:25.217713Z","shell.execute_reply":"2024-01-21T17:07:25.261948Z"},"trusted":true},"execution_count":51,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y_test_dec \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test_dec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\n","\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3453,) + inhomogeneous part."],"ename":"ValueError","evalue":"setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3453,) + inhomogeneous part.","output_type":"error"}]},{"cell_type":"code","source":"print(classification_report(y_test_dec, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-01-24T20:29:16.667082Z","iopub.execute_input":"2024-01-24T20:29:16.667847Z","iopub.status.idle":"2024-01-24T20:29:16.772549Z","shell.execute_reply.started":"2024-01-24T20:29:16.667809Z","shell.execute_reply":"2024-01-24T20:29:16.771332Z"},"trusted":true},"execution_count":25,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test_dec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:2310\u001b[0m, in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclassification_report\u001b[39m(\n\u001b[1;32m   2196\u001b[0m     y_true,\n\u001b[1;32m   2197\u001b[0m     y_pred,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2205\u001b[0m ):\n\u001b[1;32m   2206\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a text report showing the main classification metrics.\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m \n\u001b[1;32m   2208\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <classification_report>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2307\u001b[0m \u001b[38;5;124;03m    <BLANKLINE>\u001b[39;00m\n\u001b[1;32m   2308\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2310\u001b[0m     y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2312\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2313\u001b[0m         labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:87\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03mThis converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03my_pred : array or indicator matrix\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     86\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[0;32m---> 87\u001b[0m type_true \u001b[38;5;241m=\u001b[39m \u001b[43mtype_of_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my_true\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     90\u001b[0m y_type \u001b[38;5;241m=\u001b[39m {type_true, type_pred}\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/multiclass.py:345\u001b[0m, in \u001b[0;36mtype_of_target\u001b[0;34m(y, input_name)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    341\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__array__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(y[\u001b[38;5;241m0\u001b[39m], Sequence)\n\u001b[1;32m    343\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(y[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m    344\u001b[0m     ):\n\u001b[0;32m--> 345\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    346\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou appear to be using a legacy multi-label data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    347\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m representation. Sequence of sequences are no\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    348\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m longer supported; use a binary array or sparse\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    349\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m matrix instead - the MultiLabelBinarizer\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    350\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m transformer can convert to this format.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    351\u001b[0m         )\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n","\u001b[0;31mValueError\u001b[0m: You appear to be using a legacy multi-label data representation. Sequence of sequences are no longer supported; use a binary array or sparse matrix instead - the MultiLabelBinarizer transformer can convert to this format."],"ename":"ValueError","evalue":"You appear to be using a legacy multi-label data representation. Sequence of sequences are no longer supported; use a binary array or sparse matrix instead - the MultiLabelBinarizer transformer can convert to this format.","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}